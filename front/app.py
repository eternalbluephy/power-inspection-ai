import gradio as gr
import requests
import cv2
import numpy as np
import base64
import json
import time
from PIL import ImageGrab
import tkinter as tk
import os
import uuid

# åç«¯æœåŠ¡åœ°å€
API_BASE = "http://localhost:8000"
API_URL = f"{API_BASE}/api/predict"
BATCH_URL = f"{API_BASE}/api/predict_batch"
MODELS_URL = f"{API_BASE}/api/models"
REPORT_URL = f"{API_BASE}/api/report"

# ä¸´æ—¶å›¾ç‰‡ç›®å½•
TEMP_DIR = os.path.join(os.getcwd(), "temp_upload")
os.makedirs(TEMP_DIR, exist_ok=True)

# å…¨å±€ Session å¤ç”¨è¿æ¥
global_session = requests.Session()

def get_available_models():
    """ä»åç«¯è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨"""
    try:
        resp = requests.get(MODELS_URL, timeout=2)
        if resp.status_code == 200:
            data = resp.json()
            return data.get("models", [])
    except:
        return ["yolo11n.pt"] 
    return ["yolo11n.pt"]

# --- å±å¹•åŒºåŸŸé€‰æ‹©å™¨ (Tkinter Overlay) ---
class ScreenSelector:
    def __init__(self):
        self.root = None
        self.start_x = None
        self.start_y = None
        self.rect = None
        self.selection = None # (x1, y1, x2, y2)

    def select_area(self):
        self.selection = None
        self.root = tk.Tk()
        # å…¨å±é€æ˜é®ç½©
        self.root.attributes('-alpha', 0.3)
        self.root.attributes('-fullscreen', True)
        self.root.attributes('-topmost', True)
        self.root.config(bg='black')
        self.root.config(cursor="cross")
        
        # Canvas
        self.canvas = tk.Canvas(self.root, cursor="cross", bg="black")
        self.canvas.pack(fill="both", expand=True)

        self.canvas.bind("<ButtonPress-1>", self.on_press)
        self.canvas.bind("<B1-Motion>", self.on_drag)
        self.canvas.bind("<ButtonRelease-1>", self.on_release)
        self.canvas.bind("<Escape>", lambda e: self.root.destroy())

        # æç¤ºæ–‡å­—
        self.canvas.create_text(
            self.root.winfo_screenwidth()//2, 
            self.root.winfo_screenheight()//2, 
            text="è¯·æŒ‰ä½é¼ æ ‡å·¦é”®ç”»æ¡†é€‰æ‹©åŒºåŸŸ (ESCå–æ¶ˆ)", 
            fill="white", font=("Arial", 20)
        )

        self.root.mainloop()
        return self.selection

    def on_press(self, event):
        self.start_x = self.root.winfo_pointerx() - self.root.winfo_rootx()
        self.start_y = self.root.winfo_pointery() - self.root.winfo_rooty()
        # Create initial rect
        self.rect = self.canvas.create_rectangle(self.start_x, self.start_y, self.start_x, self.start_y, outline='red', width=3)

    def on_drag(self, event):
        cur_x = self.root.winfo_pointerx() - self.root.winfo_rootx()
        cur_y = self.root.winfo_pointery() - self.root.winfo_rooty()
        self.canvas.coords(self.rect, self.start_x, self.start_y, cur_x, cur_y)

    def on_release(self, event):
        end_x = self.root.winfo_pointerx() - self.root.winfo_rootx()
        end_y = self.root.winfo_pointery() - self.root.winfo_rooty()
        
        # Normalize coords
        x1 = min(self.start_x, end_x)
        y1 = min(self.start_y, end_y)
        x2 = max(self.start_x, end_x)
        y2 = max(self.start_y, end_y)
        
        # Ensure valid area
        if x2 - x1 > 10 and y2 - y1 > 10:
            self.selection = f"{x1}, {y1}, {x2}, {y2}"
        
        self.root.destroy()

def open_selector():
    selector = ScreenSelector()
    res = selector.select_area()
    return res or ""  # Return empty string if cancelled

def decode_base64_to_img(base64_str):
    if not base64_str: return None
    img_bytes = base64.b64decode(base64_str)
    nparr = np.frombuffer(img_bytes, np.uint8)
    return cv2.cvtColor(cv2.imdecode(nparr, cv2.IMREAD_COLOR), cv2.COLOR_BGR2RGB)

def draw_boxes_local(image, results):
    """
    åœ¨æœ¬åœ°ç»˜åˆ¶æ£€æµ‹æ¡†
    image: RGB numpy array
    results: list of dicts from backend
    """
    if image is None: return None
    
    # Copy to avoid modifying original if needed
    img_draw = image.copy()
    
    for r in results:
        box = r.get('box')
        cls_name = r.get('class_name')
        conf = r.get('conf')
        # Simple hash for color if class_id missing
        cls_id = r.get('class_id', hash(cls_name) % 100)
        
        if box:
            x1, y1, x2, y2 = map(int, box)
            color = random_color(cls_id)
            
            cv2.rectangle(img_draw, (x1, y1), (x2, y2), color, 2)
            
            label = f"{cls_name} {conf:.2f}"
            (w, h), _ = cv2.getTextSize(label, 0, 0.6, 1)
            cv2.rectangle(img_draw, (x1, y1 - 20), (x1 + w, y1), color, -1)
            cv2.putText(img_draw, label, (x1, y1 - 5), 0, 0.6, (255, 255, 255), 1)
            
    return img_draw

# --- 1. å•å›¾é¢„æµ‹ Pipeline ---
def predict_single(image, conf_thres, model_name):
    """
    å•å›¾é¢„æµ‹ (æœ¬åœ°æé€Ÿç‰ˆ + å¼ºåˆ¶640p):
    1. å¼ºåˆ¶ç¼©å°åˆ°é•¿è¾¹ 640
    2. ä¿å­˜ä¸´æ—¶æ–‡ä»¶
    3. ä¼ è·¯å¾„ç»™åç«¯ (Local Handoff)
    4. æœ¬åœ°ç”»å›¾
    """
    if image is None: return None, [["é”™è¯¯", "è¯·ä¸Šä¼ å›¾ç‰‡", ""]]
    
    temp_path = None
    scale = 1.0
    
    try:
        # Step 0: å¼ºåˆ¶ Resize åˆ° 640 (ç”¨æˆ·éœ€æ±‚)
        h, w = image.shape[:2]
        target_size = 640
        
        # åªè¦æœ‰ä¸€è¾¹è¶…è¿‡640ï¼Œæˆ–è€…ä¸ºäº†ç»Ÿä¸€æ ·å¼ï¼Œéƒ½resize?
        # é€šå¸¸æ˜¯ç¼©æ”¾åˆ°é•¿è¾¹ä¸º640
        if max(h, w) > target_size: 
            scale = target_size / max(h, w)
            new_w, new_h = int(w * scale), int(h * scale)
            image_processed = cv2.resize(image, (new_w, new_h), interpolation=cv2.INTER_LINEAR)
        else:
            # å¦‚æœå°äº640ï¼Œæ˜¯å¦è¦æ”¾å¤§ï¼Ÿä¸€èˆ¬ä¸å»ºè®®æ”¾å¤§
            # å¦‚æœç”¨æˆ·åšæŒ "æ”¹ä¸º640"ï¼Œè¿™é‡Œåªåšç¼©å°å¤„ç†ï¼Œä¿è¯é€Ÿåº¦
            image_processed = image

        # Step 1: ä¿å­˜åˆ°æœ¬åœ°ä¸´æ—¶æ–‡ä»¶
        filename = f"{uuid.uuid4()}.jpg"
        temp_path = os.path.join(TEMP_DIR, filename)
        
        img_bgr = cv2.cvtColor(image_processed, cv2.COLOR_RGB2BGR)
        cv2.imwrite(temp_path, img_bgr)

        # Step 2: å‘é€è¯·æ±‚
        data = {
            'conf': conf_thres, 
            'model_name': model_name, 
            'return_image': 'False',
            'image_path': temp_path 
        }
        
        resp = global_session.post(API_URL, data=data, timeout=10)
        
        if resp.status_code != 200: return image, [["API é”™è¯¯", resp.text, ""]]
        
        res = resp.json()
        results = res.get('results', [])
        
        # Step 3: åæ ‡è¿˜åŸ (è¿˜åŸå›åŸå›¾å°ºå¯¸ç”»æ¡†ï¼Œæˆ–è€…å°±åœ¨640å›¾ä¸Šç”»ï¼Ÿ)
        # æ—¢ç„¶ç”¨æˆ·è¦çœ‹ç»“æœï¼Œä¸€èˆ¬å¸Œæœ›åœ¨ä¸Šä¼ çš„åŸå›¾ä¸Šç”»ï¼Œæˆ–è€…æ˜¾ç¤ºçš„å›¾å°±æ˜¯640çš„
        # è¿™é‡Œæˆ‘ä»¬è¿”å›åŸå›¾(åŸå°ºå¯¸)ï¼Œæ‰€ä»¥è¦è¿˜åŸåæ ‡
        if scale != 1.0:
            for r in results:
                if 'box' in r:
                    r['box'] = [c / scale for c in r['box']]

        # Step 4: æœ¬åœ°ç»˜å›¾
        out_img = draw_boxes_local(image, results)
        
        # è½¬æ¢ä¸ºè¡¨æ ¼æ•°æ®
        df_data = []
        if not results:
             df_data = [["æ— ç›®æ ‡", "-", "-"]]
        else:
            for r in results:
                name = r.get('class_name', 'unknown')
                conf = f"{r.get('conf', 0):.2f}"
                box = r.get('box', [])
                df_data.append([name, conf, str(box)])

        return out_img, df_data
    except Exception as e:
        return image, [["ç³»ç»Ÿé”™è¯¯", str(e), ""]]
    finally:
        if temp_path and os.path.exists(temp_path):
            try: os.remove(temp_path)
            except: pass

# --- 2. æ‰¹é‡é¢„æµ‹ Pipeline ---
def predict_batch_pipeline(file_objs, conf_thres, model_name):
    """
    æœ¬åœ°æé€Ÿæ‰¹é‡é¢„æµ‹: ç›´æ¥å‘é€æ–‡ä»¶è·¯å¾„åˆ—è¡¨ç»™åç«¯ (Local Handoff)
    æ³¨: æ‰¹é‡å¤„ç†ä¸è¿›è¡Œå›¾ç‰‡ Resize (å› ä¸ºä¸èƒ½ä¿®æ”¹åŸæ–‡ä»¶)ï¼Œä¾èµ–åç«¯å¤„ç†
    """
    if not file_objs: return [], [["æ— æ–‡ä»¶", "-", "-"]], None
    
    # Send paths directly
    data = {
        'conf': conf_thres, 
        'model_name': model_name, 
        'return_image': 'False',
        'image_paths': file_objs 
    }
    
    try:
        resp = global_session.post(BATCH_URL, data=data, timeout=60)
 
    except Exception as e:
         return [], [["è¯·æ±‚é”™è¯¯", str(e), "-"]], None

    if resp.status_code != 200: return [], [["Backend Error", resp.text, "-"]], None
    
    try:
        result = resp.json()
        batch_results = result.get("batch_results", [])
        
        gallery_images = []
        summary_data = []
        full_results_state = [] 
        
        for idx, item in enumerate(batch_results):
            fname = item.get('filename', 'unknown')
            processed = None
            if idx < len(file_objs):
                fpath = file_objs[idx]
                if os.path.exists(fpath):
                    img_array = np.fromfile(fpath, np.uint8)
                    img_orig = cv2.imdecode(img_array, cv2.IMREAD_COLOR)
                    if img_orig is not None:
                        img_rgb = cv2.cvtColor(img_orig, cv2.COLOR_BGR2RGB)
                        res_list = item.get('results', [])
                        processed = draw_boxes_local(img_rgb, res_list)

            if processed is not None:
                label = f"{fname} ({item['count']})"
                gallery_images.append((processed, label))
                
                count = item.get('count', 0)
                res_list = item.get('results', [])
                stats = {}
                for r in res_list:
                    cname = r.get('class_name', 'unknown')
                    stats[cname] = stats.get(cname, 0) + 1
                
                detail_str = ", ".join([f"{k}:{v}" for k,v in stats.items()]) if stats else "æ— ç›®æ ‡"
                summary_data.append([fname, count, detail_str])
                
                full_results_state.append({
                    "image": processed,
                    "filename": fname,
                    "results": res_list
                })
            
        return gallery_images, summary_data, full_results_state
        
    except Exception as e:
        return [], [["ç³»ç»Ÿé”™è¯¯", str(e), "-"]], None
        
# --- äº¤äº’äº‹ä»¶å¤„ç† ---
def on_select_gallery(evt: gr.SelectData, state):
    # evt.index æ˜¯ gallery ä¸­è¢«é€‰ä¸­çš„ç´¢å¼•
    if not state or evt.index >= len(state): return None, [["æ— æ•°æ®", "-", "-"]]
    
    selected = state[evt.index]
    img = selected['image']
    results = selected['results']
    
    # æ„å»ºè¯¦ç»†è¡¨æ ¼
    df_data = []
    if not results:
         df_data = [["æ— ç›®æ ‡", "-", "-"]]
    else:
        for r in results:
            name = r.get('class_name', 'unknown')
            conf = f"{r.get('conf', 0):.2f}"
            box = r.get('box', [])
            box_str = f"[{int(box[0])}, {int(box[1])}, {int(box[2])}, {int(box[3])}]"
            df_data.append([name, conf, box_str])
            
    return img, df_data

def on_select_dataframe(evt: gr.SelectData, state):
    # evt.index[0] æ˜¯ dataframe è¡Œç´¢å¼• (å¯¹äº Dataframe, index æ˜¯ (row, col))
    row_idx = evt.index[0]
    return on_select_gallery(type('obj', (object,), {'index': row_idx}), state)

    # finally:
    #     for _, f in uploaded_files: f.close()

def random_color(id):
    import colorsys
    h = (((id << 2) ^ 0x937151) % 100) / 100.0
    s = (((id << 3) ^ 0x315793) % 100) / 100.0
    r, g, b = colorsys.hsv_to_rgb(h, s, 1)
    return int(r * 255), int(g * 255), int(b * 255)

# --- 3. å±å¹•å®æ—¶æµ Pipeline ---
def predict_screen_stream(conf_thres, model_name, roi_str):
    """
    Generator function that captures screen and yields predicted frames
    Using Optimized Strategy: Downscale Request -> Local Draw
    """
    bbox = None
    if roi_str and "," in roi_str:
        try:
            # Parse "x1,y1,x2,y2"
            parts = list(map(int, roi_str.replace(" ", "").split(',')))
            if len(parts) == 4:
                bbox = tuple(parts)
        except:
            print("Invalid ROI format, using full screen")

    # Reuse session for speed
    session = requests.Session()
    
    while True:
        try:
            # 1. Capture Screen
            screen = ImageGrab.grab(bbox=bbox) 
            img_orig = np.array(screen) # RGB
            h_orig, w_orig = img_orig.shape[:2]
            
            # 2. Downscale for faster upload/inference
            # 640 is typical YOLO size, no need to send 4K screen
            scale_size = 640
            scale = scale_size / max(h_orig, w_orig)
            w_new, h_new = int(w_orig * scale), int(h_orig * scale)
            img_resized = cv2.resize(img_orig, (w_new, h_new))

            # 3. Encode (Faster JPEG)
            img_bgr_small = cv2.cvtColor(img_resized, cv2.COLOR_RGB2BGR)
            # JPEG Quality 60 is enough for detection
            _, img_encoded = cv2.imencode('.jpg', img_bgr_small, [int(cv2.IMWRITE_JPEG_QUALITY), 60])
            
            files = {'file': ('image.jpg', img_encoded.tobytes(), 'image/jpeg')}
            data = {'conf': conf_thres, 'model_name': model_name, 'return_image': 'False'}
            
            # 4. Request (No return image)
            resp = session.post(API_URL, files=files, data=data, timeout=1)
            
            if resp.status_code == 200:
                res = resp.json()
                results = res.get('results', [])
                
                # 5. Draw on Original High-Res Image (Local CPU)
                # Map coordinates back: x_orig = x_pred / scale
                for r in results:
                    box = r.get('box')
                    cls_name = r.get('class_name')
                    conf = r.get('conf')
                    cls_id = r.get('class_id', 0)
                    
                    if box:
                        # Rescale box back to original screen size
                        x1 = int(box[0] / scale)
                        y1 = int(box[1] / scale)
                        x2 = int(box[2] / scale)
                        y2 = int(box[3] / scale)
                        
                        color = random_color(cls_id)
                        
                        # Draw Rect
                        cv2.rectangle(img_orig, (x1, y1), (x2, y2), color, 3)
                        
                        # Draw Label
                        label = f"{cls_name} {conf:.2f}"
                        (w, h), _ = cv2.getTextSize(label, 0, 0.6, 1)
                        cv2.rectangle(img_orig, (x1, y1 - 20), (x1 + w, y1), color, -1)
                        cv2.putText(img_orig, label, (x1, y1 - 5), 0, 0.6, (255, 255, 255), 1)
            
            yield img_orig
            
        except Exception as e:
            # print(f"Screen stream error: {e}")
            yield img_orig # Return raw screen if fail

# --- 4. è·å–æŠ¥è¡¨ ---

# --- 4. è·å–æŠ¥è¡¨ ---
# å…¨å±€ç¼“å­˜å˜é‡ï¼Œé¿å…é‡å¤è¯·æ±‚é™æ€èµ„æº
REPORT_CACHE = {}

def get_report_image(model_dropdown,report_type):
    # 1. ä¼˜å…ˆæ£€æŸ¥ç¼“å­˜
    cache_key = f"{model_dropdown}_{report_type}"
    if cache_key in REPORT_CACHE:
        return REPORT_CACHE[cache_key], "è·å–æˆåŠŸ (æ¥è‡ªç¼“å­˜)"

    # report_type: loss, confusion_matrix, pr_curve
    url = f"{REPORT_URL}/{report_type}"
    params = {"model_name": model_dropdown} if model_dropdown else {}
    
    try:
        # 2. ä½¿ç”¨ global_session å¤ç”¨è¿æ¥
        resp = global_session.get(url, params=params, timeout=5)
        
        if resp.status_code == 200:
            # Response is raw image bytes
            nparr = np.frombuffer(resp.content, np.uint8)
            img = cv2.imdecode(nparr, cv2.IMREAD_COLOR)
            img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
            
            # 3. å†™å…¥ç¼“å­˜
            REPORT_CACHE[cache_key] = img_rgb
            
            return img_rgb, "è·å–æˆåŠŸ (æœåŠ¡ç«¯)"
        else:
            return None, f"è·å–å¤±è´¥: {resp.json().get('detail')}"
    except Exception as e:
        return None, str(e)

# --- æ„å»º UI ---
with gr.Blocks(title="ç”µåŠ›å·¡æ£€ç³»ç»Ÿ v2.0", theme=gr.themes.Soft()) as demo:
    gr.Markdown("# âš¡ ç”µåŠ›å·¡æ£€å›¾åƒæ™ºèƒ½æ£€æµ‹ç³»ç»Ÿ")
    
    # å…¨å±€è®¾ç½®åŒº
    with gr.Row(variant="panel"):
        model_choices = get_available_models()
        default_model = model_choices[0] if model_choices else "yolo11n.pt"
        model_dropdown = gr.Dropdown(model_choices, value=default_model, label="é€‰æ‹©æ¨¡å‹", scale=2, allow_custom_value=True)
        conf_slider = gr.Slider(0.0, 1.0, value=0.4, label="ç½®ä¿¡åº¦é˜ˆå€¼", scale=2)
        refresh_btn = gr.Button("ğŸ”„", scale=0, min_width=50)
        
        def refresh_models():
            ch = get_available_models()
            return gr.Dropdown(choices=ch, value=ch[0] if ch else "")
        refresh_btn.click(refresh_models, outputs=[model_dropdown])

    with gr.Tabs():
        # --- Tab 1: å•å›¾æ£€æµ‹ ---
        with gr.Tab("ğŸ“· å•å›¾æ£€æµ‹"):
            with gr.Row():
                with gr.Column():
                    t1_input = gr.Image(type="numpy", label="Input")
                    t1_btn = gr.Button("å¼€å§‹æ£€æµ‹", variant="primary")
                with gr.Column():
                    t1_output = gr.Image(type="numpy", label="æ£€æµ‹ç»“æœ")
                    # t1_json = gr.JSON(label="è¯¦ç»†æ•°æ®")
                    t1_table = gr.Dataframe(
                        headers=["ç±»åˆ«", "ç½®ä¿¡åº¦", "åæ ‡ä½ç½®"],
                        datatype=["str", "str", "str"],
                        label="æ£€æµ‹è¯¦æƒ…",
                        interactive=False
                    )
            
            t1_btn.click(predict_single, [t1_input, conf_slider, model_dropdown], [t1_output, t1_table])

        # --- Tab 2: æ‰¹é‡æ£€æµ‹ ---
        with gr.Tab("ğŸ“‚ æ‰¹é‡æ£€æµ‹"):
            gr.Markdown("æ”¯æŒæ‰¹é‡ä¸Šä¼ å¤šå¼ å›¾ç‰‡è¿›è¡Œå¤„ç†ã€‚ç‚¹å‡»å¤„ç†ç»“æœå›¾ç‰‡ä¸è¡¨æ ¼è¡Œå¯è¿›è¡Œè”åŠ¨æŸ¥çœ‹ã€‚")
            with gr.Row():
                with gr.Column(scale=1):
                    # file_count="multiple" returns list of file paths
                    t2_input = gr.File(file_count="multiple", type="filepath", label="é€‰æ‹©å¤šå¼ å›¾ç‰‡")
                    t2_btn = gr.Button("æ‰¹é‡å¤„ç†", variant="primary")
                with gr.Column(scale=2):
                    # æ€»è§ˆåŒºåŸŸ
                    with gr.Group():
                        gr.Markdown("### 1. ç»“æœæ¦‚è§ˆ (Gallery & Summary)")
                        with gr.Row():
                            t2_gallery = gr.Gallery(label="æ‰€æœ‰å›¾ç‰‡", columns=4, height=300, allow_preview=False)
                            t2_table_sum = gr.Dataframe(
                                headers=["æ–‡ä»¶å", "ç›®æ ‡æ•°", "ç»Ÿè®¡"],
                                datatype=["str", "number", "str"],
                                label="ç»Ÿè®¡æŠ¥å‘Š (ç‚¹å‡»è¡ŒæŸ¥çœ‹)",
                                interactive=False
                            )

                    # è¯¦æƒ…åŒºåŸŸ
                    gr.Markdown("### 2. é€‰ä¸­è¯¦æƒ… (Selected Detail)")
                    with gr.Row():
                        t2_selected_img = gr.Image(label="å½“å‰é€‰ä¸­å›¾ç‰‡", type="numpy", height=500)
                        t2_table_detail = gr.Dataframe(
                            headers=["ç±»åˆ«", "ç½®ä¿¡åº¦", "åæ ‡"],
                            datatype=["str", "str", "str"],
                            label="å½“å‰å›¾ç‰‡æ£€æµ‹æ•°æ®",
                            interactive=False
                        )
            
            # State
            t2_state = gr.State()

            # Events
            t2_btn.click(predict_batch_pipeline, 
                         [t2_input, conf_slider, model_dropdown], 
                         [t2_gallery, t2_table_sum, t2_state])
            
            # Linkage
            t2_gallery.select(on_select_gallery, inputs=[t2_state], outputs=[t2_selected_img, t2_table_detail])
            t2_table_sum.select(on_select_dataframe, inputs=[t2_state], outputs=[t2_selected_img, t2_table_detail])

        # --- Tab 3: å±å¹•å®æ—¶æ£€æµ‹ ---
        with gr.Tab("ğŸ–¥ï¸ å±å¹•å®æ—¶æ£€æµ‹"):
            gr.Markdown("å®æ—¶æ•è·ç”µè„‘å±å¹•è¿›è¡Œæ£€æµ‹ (Screen Capture)")
            
            with gr.Row():
                with gr.Column(scale=4):
                    roi_input = gr.Textbox(
                        label="æ•è·åŒºåŸŸ (x1, y1, x2, y2)", 
                        placeholder="ä¾‹å¦‚: 100, 100, 800, 600 (ç•™ç©ºåˆ™å…¨å±)",
                        info="è¯·è¾“å…¥åæ ‡æˆ–ä½¿ç”¨å³ä¾§æŒ‰é’®é€‰å–"
                    )
                with gr.Column(scale=1):
                    select_btn = gr.Button("âœ‚ï¸ æ¡†é€‰åŒºåŸŸ", min_width=80)
            
            # Selector Action
            select_btn.click(open_selector, outputs=[roi_input])

            with gr.Row():
                with gr.Column(scale=1):
                    start_btn = gr.Button("â–¶ï¸ å¼€å§‹å±å¹•æ•è·", variant="primary")
                    stop_btn = gr.Button("â¹ï¸ åœæ­¢æ•è·")
                with gr.Column(scale=3):
                    stream_output = gr.Image(label="å±å¹•æ£€æµ‹æµ", interactive=False)
            
            # Event: Click start to trigger generator, Click stop to cancel
            stream_event = start_btn.click(
                predict_screen_stream, 
                [conf_slider, model_dropdown, roi_input], 
                [stream_output]
            )
            stop_btn.click(fn=None, cancels=[stream_event])

        # --- Tab 4: æ¨¡å‹è¯„ä¼° ---
        with gr.Tab("ğŸ“Š æ¨¡å‹è¯„ä¼°"):
            gr.Markdown("æŸ¥çœ‹å½“å‰æ¨¡å‹åœ¨è®­ç»ƒé›†ä¸Šçš„è¡¨ç°")
            with gr.Row():
                btn_cm = gr.Button("æ··æ·†çŸ©é˜µ (Confusion Matrix)")
                btn_pr = gr.Button("PR æ›²çº¿")
                btn_loss = gr.Button("è®­ç»ƒ Loss")
            
            with gr.Row():
                report_img = gr.Image(label="è¯„ä¼°å›¾è¡¨")
                report_msg = gr.Textbox(label="çŠ¶æ€", interactive=False)
            
            btn_cm.click(lambda m: get_report_image(m, "confusion_matrix"), inputs=[model_dropdown], outputs=[report_img, report_msg])
            btn_pr.click(lambda m: get_report_image(m, "pr_curve"), inputs=[model_dropdown], outputs=[report_img, report_msg])
            btn_loss.click(lambda m: get_report_image(m, "loss"), inputs=[model_dropdown], outputs=[report_img, report_msg])
            
        # --- Tab 5: ç³»ç»Ÿè¯´æ˜ ---
        with gr.Tab("â„¹ï¸ å…³äºç³»ç»Ÿ"):
            gr.Markdown("""
            ## ğŸ“ è¯¾è®¾é¡¹ç›®æ¼”ç¤ºç³»ç»Ÿ
            
            æœ¬ç³»ç»Ÿé›†æˆäº†ä¸¤ä¸ªå…³é”®æŠ€æœ¯æ¨¡å—ï¼š
            
            1.  **é«˜ç²¾åº¦ç¼ºé™·æ£€æµ‹æ¨¡å‹ (Task 1)**
                *   æ¨¡å‹æ¶æ„: YOLOv11s
                *   æ¡†æ¶: PyTorch 1.12+ (Training), ONNX (Intermediate)
                *   æ•°æ®é›†: ç»ç¼˜å­ç¼ºé™·æ•°æ®é›† (VOC/YOLOæ ¼å¼)
                *   å¢å¼º: Mosaic, Mixup, HSV Augmentation
            
            2.  **é«˜æ€§èƒ½æ¨ç†åŠ é€Ÿ (Task 2)**
                *   æ¨ç†å¼•æ“: NVIDIA TensorRT 8.x
                *   ç²¾åº¦ä¼˜åŒ–: FP16 / INT8 Quantization
                *   æœåŠ¡åŒ–: C++ Inference Service (Backend) + FastAPI
            
            **å¼€å‘æ ˆ:**
            *   Frontend: Gradio / Python
            *   Backend: FastAPI / C++ / TensorRT
            *   CV Lib: OpenCV 4.x
            """)

if __name__ == "__main__":
    demo.launch(server_name="0.0.0.0", server_port=7860)